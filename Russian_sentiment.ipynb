{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "MsL7NJiU4d1u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlD7tkP61yJP"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"senylar/sis-text-class\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess(text):\n",
        "    # Превращаем в нижний регистр\n",
        "    text = text.lower()\n",
        "    # Удаляем всё, кроме букв и пробелов\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Разбиваем на слова\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "print(preprocess(\"Очень хороший сервис! Приду ещё.\"))\n",
        "# ['очень', 'хороший', 'сервис', 'приду', 'ещё']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNAiE6wv10eZ",
        "outputId": "4245f22e-1186-4a95-cb25-2ea71ae9d81f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['очень', 'хороший', 'сервис', 'приду', 'ещё']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')"
      ],
      "metadata": {
        "id": "l25_Zk7V4aUu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = train['text']\n",
        "train_sample = train.sample(n=50000, random_state=42)\n",
        "texts = train_sample['text'].tolist()\n",
        "labels = train_sample['sentiment'].tolist()\n",
        "\n",
        "tokenized_texts = [preprocess(t) for t in texts]\n",
        "\n",
        "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "for tokens in tokenized_texts:\n",
        "    for token in tokens:\n",
        "        if token not in vocab:\n",
        "            vocab[token] = len(vocab)"
      ],
      "metadata": {
        "id": "EaR6N_fV9pIM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab, max_len=50):\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "        self.label_map = {\"negative\": 2, \"neutral\": 0, \"positive\": 1}\n",
        "\n",
        "        self.tokenized = [self.encode(preprocess(t)) for t in texts]\n",
        "        self.labels = labels\n",
        "    def encode(self, tokens):\n",
        "        ids = [self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in tokens]\n",
        "        return ids[:self.max_len] + [self.vocab[\"<PAD>\"]] * (self.max_len - len(ids))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.tokenized[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = SentimentDataset(texts, labels, vocab)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Rmbeahct3sn0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # вот тут\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)            # [B, T] -> [B, T, E]\n",
        "        _, h = self.rnn(x)               # [B, T, E] -> [1, B, H]\n",
        "        out = self.fc(h.squeeze(0))      # [B, H] -> [B, C]\n",
        "        return out\n",
        "\n",
        "# Инициализация\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SentimentModel(vocab_size=len(vocab), embedding_dim=250, hidden_dim=64, num_classes=3)\n",
        "model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)"
      ],
      "metadata": {
        "id": "1YOzxiW3BC5J"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Тренировка\n",
        "for epoch in tqdm(range(4)):\n",
        "  for X_batch, y_batch in train_loader:\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(X_batch)\n",
        "    loss = loss_fn(out, y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "  preds = torch.argmax(out, dim=1)\n",
        "  acc = (preds == y_batch).float().mean()\n",
        "  print(f\"Эпоха {epoch}, loss={loss.item():.4f}, accuracy={acc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "ZGkLfrrV3vXe",
        "outputId": "8d3a7dd8-069d-425d-894f-69b1b8a3316a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 1/15 [00:45<10:38, 45.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 0, loss=0.6108, accuracy=0.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 2/15 [01:31<09:53, 45.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 1, loss=0.4702, accuracy=0.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 3/15 [02:16<09:07, 45.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 2, loss=0.4279, accuracy=0.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 4/15 [03:02<08:21, 45.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 3, loss=0.0487, accuracy=1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 4/15 [03:16<09:01, 49.23s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-2b15b643fd94>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"sentiment_model.pth\")"
      ],
      "metadata": {
        "id": "I1oJfnG0KyLQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(tokens, vocab, max_len=120):\n",
        "    ids = [vocab.get(t, vocab[\"<UNK>\"]) for t in tokens]\n",
        "    if len(ids) < max_len:\n",
        "        ids += [vocab[\"<PAD>\"]] * (max_len - len(ids))\n",
        "    else:\n",
        "        ids = ids[:max_len]\n",
        "    return ids\n",
        "\n",
        "\n",
        "def predict(text, model, vocab):\n",
        "    model.eval()  # переводим в режим оценки\n",
        "    tokens = preprocess(text)                     # Токенизация\n",
        "\n",
        "    encoded = encode(tokens, vocab)               # Преобразуем в индексы\n",
        "    tensor = torch.tensor([encoded]).to(device)  # Преобразуем в тензор и на нужное устройство\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(tensor)                    # Получаем логиты\n",
        "        pred = torch.argmax(output, dim=1).item() # Выбираем класс с максимальной вероятностью\n",
        "\n",
        "    # Словарь отображения метки в строку\n",
        "    idx2label = {0: \"neutral\", 1: \"positive\", 2: \"negative\"}\n",
        "    return idx2label[pred]\n",
        "\n",
        "\n",
        "# --- 7. Ввод с клавиатуры ---\n",
        "for _ in range(3):\n",
        "    user_text = input(\"Введите текст (или 'выход' для завершения): \")\n",
        "    if user_text.strip().lower() == \"выход\":\n",
        "        break\n",
        "    print(\"Оценка:\", predict(user_text, model, vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW61h7Kb30fh",
        "outputId": "479abff2-093c-4426-e55e-694414c477ee"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Введите текст (или 'выход' для завершения): Самый лучший отель в жизни\n",
            "Оценка: positive\n",
            "Введите текст (или 'выход' для завершения): Этот отель - это самый ужасный и худший из всех\n",
            "Оценка: negative\n",
            "Введите текст (или 'выход' для завершения): Нормас отель\n",
            "Оценка: neutral\n"
          ]
        }
      ]
    }
  ]
}